{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2196df04",
   "metadata": {},
   "source": [
    "- https://eugeneyan.com/writing/simplicity/\n",
    "- https://towardsdatascience.com/the-building-blocks-of-a-modern-data-platform-92e46061165\n",
    "- https://levelup.gitconnected.com/idempotency-in-api-design-bc4ea812a881\n",
    "- https://medium.com/swlh/an-efficient-framework-to-approach-system-design-problems-cf058f614a84\n",
    "- https://betterprogramming.pub/9-real-challenges-that-data-engineers-face-9c9adc2dac97\n",
    "- https://lakefs.io/chaos-data-engineering/\n",
    "https://specbranch.com/posts/one-big-server/\n",
    "- https://zendesk.engineering/zen-and-the-art-of-reliability-f42fa7e64849\n",
    "- https://blog.mi.hdm-stuttgart.de/index.php/2022/03/03/cascading-failures-in-large-scale-distributed-systems/\n",
    "- https://future.a16z.com/emerging-architectures-modern-data-infrastructure/\n",
    "- https://rumproarious.com/2019/12/15/performance-matters/\n",
    "- https://rumproarious.com/2019/12/11/helpful-numbers-for-designing-technical-systems/\n",
    "- https://medium.com/cermati-tech/on-distributed-systems-setup-and-architecture-planning-8ad9954fe2c2\n",
    "- https://medium.com/swlh/how-i-scaled-a-software-systems-performance-by-35-000-6dacd63732df\n",
    "- https://future.com/podcasts/evolution-of-data-architectures/\n",
    "- https://kislayverma.com/software-architecture/the-mechanics-of-software-evolution/\n",
    "= https://www.infoq.com/articles/what-software-architecture/\n",
    "- https://www.youtube.com/watch?v=St0_Nar5meM&list=WL&index=5&ab_channel=RakeshJaiswal\n",
    "- https://learningdaily.dev/the-complete-guide-to-system-design-in-2022-31e319a91cac\n",
    "- https://www.hillelwayne.com/post/performance-matters/\n",
    "https://www.microsoft.com/en-us/research/video/minimizing-faulty-executions-distributed-systems/\n",
    "- https://kislayverma.com/software-architecture/on-scalable-software/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd91a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arcitecture consideration and tradeoffs\n",
    "- how to split work and data\n",
    "- how to coordinate state\n",
    "- robustness and managing failure\n",
    "- scale bottlenecks and efficiency\n",
    "- understanbility and predictiability\n",
    "- maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "__RAW : ETL best practices\n",
    "\n",
    "If you are starting with Big Data it is common to feel overwhelmed by the large number of tools, frameworks and options to choose from. In this article, I will try to summarize the ingredients and the basic recipe to get you started in your Big Data journey. My goal is to categorize the different tools and try to explain the purpose of each tool and how it fits within the ecosystem.\n",
    "https://www.dataopsmanifesto.org/\n",
    "best practices don't guarantee safe actions\n",
    "These are the pillar I think are critical for data pipeline:\n",
    "Requirements\n",
    "Check the volume of your data?\n",
    " how much do you have and how long do you need to store for?\n",
    " Check the temperature! of the data, it loses value over time, so how long do you need to store the data for? how many storage layers(hot/warm/cold) do you need? \n",
    "can you archive or delete data?\n",
    "What type of data is your storing? which formats do you use? What type is your data? Relational? Graph? Document? Do you have a schema to enforce?\n",
    "do you have any legal obligations? \n",
    "how fast do you need to ingest the data? how fast do you need the data available for querying? \n",
    "What type of queries are you expecting? OLTP or OLAP? \n",
    "What are your infrastructure limitations?\n",
    "Latency vs throughput Batch vs stream\n",
    "\n",
    "12.Think about privacy\n",
    "Data retention policy. Also archive older data, e.g. move to Glacier from S3. dark data spotify linkedin privacy by design https://www.youtube.com/watch?v=DnzS2ht_ZtI\n",
    "\n",
    "Data Warehouse vs Data Lake\n",
    "\n",
    "Simplicity\n",
    "Any technology added to one's stack by default increases complexity of the stack. A lot of tools that are being built either as open source software or as commercial software draw inspiration from extremely large scale systems built at companies like Google.\n",
    "\n",
    "While there might be several great ideas incorporated into these tools that have universal utility, for the most part these tools come bundled with a fair bit of unwarranted complexity.\n",
    "And sadly, the current state of these tooling is such that they offer an all-or-nothing proposition - we as consumers aren't in the position to pick or choose only what we truly require. The \"sane default\" offered by many of these tools is now an order of magnitude more elaborate than what's truly warranted for most use-cases.\n",
    "- Cost Efficiency.\n",
    "Requirements\n",
    "\n",
    "File Format \n",
    "Deletes\n",
    "\n",
    "Schema on read vs schema on write\n",
    "\n",
    "Reproducible: \n",
    "    \n",
    "    \n",
    "- Idempotent deterministic, so we can freely rerun the jobs without any side effects (This property is going to save a lot of time).\n",
    "requests link1 link2\n",
    "data pipelines functional data engineering, functional data engineering2\n",
    "seeds\n",
    "Reusable, We believe a foundational aspect of analytic insight manufacturing efficiency is to avoid the repetition of previous work by the individual or team. which can be obtained by parametrizable components. \n",
    "Always have schema\n",
    "Fault-Tolerant and Future proof: \n",
    "backfilling, versioning \n",
    "Should be recovered fast: \n",
    "- vertically persist intermediate data between transformation \n",
    "- horizontally partition the input data\n",
    "Human fault-tolerant (A call for sanity in nosql)\n",
    "Robustness - can system perform withing predetermined expected boundaries\n",
    "like adding removing or changing tasks Handle fail-over and schema change in the data. Define the logic to handle late-arriving data, need to handle to a scenario of event_based time or arrival_time.\n",
    "timeout beyond certain wait, a success result is unlikely\n",
    "retry: Many faults are transient and may self-correct after a short delay link1\n",
    "Resilience - can it adapt when the capacity to work is exceeded. \n",
    "fallback things still fail plan what you will do when that happens \n",
    "define priority order for every job (low, medium, high) \n",
    "cache link1\n",
    "Pressure link1,link2\n",
    "Avoiding cascading failures\n",
    "Service degradation & fallback\n",
    "Rejection\n",
    "Observability\n",
    " transparent where data resides what it means and where it flows\n",
    "Our goal is to have performance, security, quality measures etc that are monitored continuously to detect unexpected variation and generate operational statistics.\n",
    "- Monitoring link1, link2\n",
    "- health check link1, link2\n",
    "- Data Lineage\n",
    "\n",
    "Metadata\n",
    "wework, paypal, lyft, netflix google,\n",
    "https://engineering.linkedin.com/blog/2019/data-hub\n",
    "\n",
    "10. Scaleability\n",
    "Partitioning column https://www.youtube.com/watch?v=DoaAk0J9T3k\n",
    "Incremental\n",
    "Compression\n",
    "aggregate core tables\n",
    "\n",
    "13.dATA QUALITY\n",
    "One important aspect in Big Data, often ignore is data quality and assurance. Companies loose every year tons of money because of data quality issues. The problem is that this is still an immature field in data science, developers have been working on this area for decades and they have great test frameworks and methodologies such BDD or TDD, but how do you test your pipeline?\n",
    "\n",
    "Analytic pipelines should be built with a foundation capable of automated detection of abnormalities and security issues in code, configuration, and data, and should provide continuous feedback to operators for error avoidance.\n",
    "This can be achieved by doing some basic quality checks in the data after each ETL job like count the number of rows, null checks, etc.Data reliability De-duplicate the events early, defining the definition of duplicate.\n",
    "TIMELY CORRECTNESS COMPLETENESS CONSISTENCY\n",
    "https://www.youtube.com/watch?v=fXHdeBnpXrg\n",
    "https://www.youtube.com/watch?v=U63TmQPS9Z8\n",
    "How leading companies scale AI\n",
    "Five key principles to build an AI production platformtowardsdatascience.com\n",
    "https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md\n",
    "https://towardsdatascience.com/data-observability-how-to-fix-your-broken-data-pipelines-3314c4fbd0d6\n",
    "https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/\n",
    "https://medium.com/@i.gorton/six-rules-of-thumb-for-scaling-software-architectures-a831960414f9\n",
    "https://towardsdatascience.com/safely-rolling-out-ml-models-to-production-13e0b8211a2f\n",
    "\n",
    "---\n",
    "https://towardsdatascience.com/how-leading-companies-scale-ai-4626189faed2\n",
    "https://rumproarious.com/2019/12/15/performance-matters/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e82c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal split aggregate high resolution data store aggregation\n",
    "incremental processesing i wouldnt want to build increment processing on top of the overwrite and i would like idempotent\n",
    "\n",
    "snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more frustrating\" hadley wickham\n",
    "\n",
    "    from distributed processing point of view the right tool will largely come down to what existing systems are in place that you need to interact with. for people working in a hadoop ecosystem the abiilty to easily load the output to diffent tools and interact with shared libraries may"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "you need stron orchestration that can manage dependency\n",
    "\n",
    "\n",
    "lazy data retrival is great untill you overload the database or you need to rerun it\n",
    "\n",
    "\n",
    "\n",
    "incremental vs read it all\n",
    "\n",
    "\n",
    "do we need real time everyting\n",
    "\n",
    "result now vs 2 hours today number or yestardays etc\n",
    "\n",
    "\n",
    "\n",
    "backfill , revert serving dataset to old fix bug remove faulty datasets done\n",
    "\n",
    "\n",
    "late arriving fact are common partition by event processing time, partition prunning is lost when filtering on that event.\n",
    "\n",
    "\n",
    "mitigation mechanism can be\n",
    "- sub-partition on event time\n",
    "- instruct people to aplly predicates on the partitioned fields\n",
    "- rely on execution engine optimization\n",
    "- pivot on event time on fact tables\n",
    "--\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "partition is more effetive on low cardinality for example boolean gender dates  etc sorted is more effective with high caridinality for example email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze- raw ingestion and history (turn off stats collection)\n",
    "silver - filtedred cleaned augmented (optimized z order by merge join keys between bronze and silver, trun optimized writes on, restructure columns to account for data skipping index columns)\n",
    "gold - buisness level aggregate    (optimize z order by join keys or common high cardinality query predicate, turned optimized writes, enable delata cache, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "we want ability to read consistd data , and read incrementaly from large table \n",
    "- ability to rollback in case of bad writes\n",
    "- ability to replay historical data\n",
    "- ability to handle late arriving data\n",
    "\n",
    "\n",
    "\n",
    "architectual pricipals\n",
    "- build decoupled systems\n",
    "- data store process store analyze answer\n",
    "- the right tooll for thee job latency throuput accfes pattersn data structures\n",
    "- leverage managed and serverless servisses\n",
    "-be cost conscious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "design considerations\n",
    "- restricted data access allow for a more efficient implementation\n",
    "- restricting data. access alsop restrict available operations\n",
    "- temporary data conversion to a different format may be required\n",
    "- design for performance must be influenced by measurements \n",
    "- measuring performance requires a working program\n",
    "- solution prototypes , mocks and expiriments\n",
    "\n",
    "\n",
    "\n",
    "architecture how do we know when one style of building is more appropriate than another (building)\n",
    "\n",
    "\n",
    "what do we expect from good architecture reliability extendability maintaibnability recoverability integrity , performability , scalability, security, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "make iteasy for developers to make the right desicions (correctness by default)\n",
    "\n",
    "you can have a second computer once you have showned you nkow how to use the first one\n",
    "\n",
    "\n",
    "\n",
    "the scale consist of 5 criteria\n",
    "code , configuration, data, and artifacts, environments, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceae1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture is the art of layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0 co\n",
    "\n",
    "branching logic for input check for models\n",
    "- check input if it pass run models else check if input run heuristics else error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular software\n",
    "- no crash\n",
    "- test passed (unit test integration tests regressions test)\n",
    "- 80 confidient in qualitu of app\n",
    "\n",
    "ml software\n",
    "- no crash\n",
    "- test passed (unit test integration tests regressions test, distribution testss, model back tests)\n",
    "- accuracy precision recall\n",
    "- 20 confidient in qualitu of app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion - (data valudation)> preprocessing -(train test validation)> training ->< evaluation -> deplotment - > prodicution\n",
    "data collection & versioning -> feature store -> training orchestraction -> expiriment management -> packageing & deployment -> model serving        -> model monitoring\n",
    "dvc wiegith and biases etc     feast clearml    kubeflow clearml etc         mlflow wieght and biases     mlflow wieght and biases fastapi kfserving seldon     aporia fiddoler\n",
    "\n",
    "\n",
    "\n",
    "data  -> features - > model -> buisness decisions\n",
    "\n",
    "model building -> model evaluation and epxirimentation -> productionizing model -> testing -> deployment -> monitoring and obserability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "when to use what model\n",
    "rule engine\n",
    "    - poor against adaptive attacks\n",
    "    - need to mainitain rules constantncly\n",
    "    - easier and quite effective\n",
    "    + intuitive\n",
    "    + flexibile\n",
    "    + capture unique behavior\n",
    "    - require domain knowlege\n",
    "    - difficult to tweak ex isting. rules\n",
    "supervised ml\n",
    "    - need alrge amount of labeld data\n",
    "    - have difficulties detecting unknown attacks\n",
    "    - take quite a lot of time\n",
    "    \n",
    "unsupervised machines learning \n",
    "    + auto laebl generagion\n",
    "    - autu rules generation\n",
    "    - detection of unknown attacks\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "reality\n",
    "mlops is hard becaause once you try to put a system in place around ml model the reality start to set in. the whole point of ml ops is to make some model lifecycle productionized and hardened, ready for the real world without someone constantly babysisng\n",
    "\n",
    "puting theses guardrial around the system always bring rocks up out of the water:\n",
    "-tests will always show you over comprex and error prone code\n",
    "- dag will show you the critical depepndecies and complexities, some of which may be out of your control.\n",
    "- click the same run button and getting the same output from the same input require incredicble attention to detains.\n",
    "\n",
    "\n",
    "\n",
    "model and data management\n",
    "one of the seamly easyy parts is tracking models runs metrics and data used. making the model approacble and usable means the knowledge surrounding a ml model cant reside with single person.\n",
    "\n",
    "\n",
    "approachability\n",
    "probably on of the biggest hurdeles in mlops is the approachability of codebases and model/. this is usually because the general complexity of data, feature engineering the model and the output. it can be overwhelming task for even a seasoned engineer to be droped in undocumented messy ml pipeline that has no devops built into it. the complexity and dependecies that pipeline thatt aboce:\n",
    "- dag and visualizations\n",
    "- functional and testsable\n",
    "- broken in to logical components\n",
    "- scalable and runnable\n",
    "- documented\n",
    "\n",
    "\n",
    " making a model from research to production is hard\n",
    " jupyter -> rewrite as python java -> data changes -> scripts broken -> feature different in production -> data leakage -> model in production -> feature changes -> preformance drift\n",
    "    \n",
    "    \n",
    "feedback loop\n",
    "    know when model fails\n",
    "        - ground truth, mneed to monitor complex satatistical summaries\n",
    "        - distribution etc\n",
    "        - often model speicifc\n",
    "    quickly   find the root caus\n",
    "        - a model is one part of the inference pipline so you need global view of the pipeline jungle to see where the root issue may be\n",
    "        \n",
    "    close the loop\n",
    "        - inteligent detection and alerting to pre emptivly identify issues and trigger remediation\n",
    "        - execute retrain fallback modesl and human intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
