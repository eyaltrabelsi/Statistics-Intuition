{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f046e76",
   "metadata": {},
   "source": [
    "- https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html\n",
    "- https://towardsdatascience.com/machine-learned-model-serving-at-scale-86f6220132c8\n",
    "- https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns\n",
    "- https://deepnote.com/@deepnote/Supercharge-Your-Shallow-ML-Models-With-Hummingbird-wzzRxFNtR1OvSEry0nq-wA\n",
    "- https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html\n",
    "- https://towardsdatascience.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving-fcfc75c4a362\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c95dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "As a Data Scientist, I want to...\tso that...\n",
    "serve standard models (scikit-learn, TensorFlow, PyTorch, XGBoost etc.) with minimum effort\tI don't have to resolve common issues by myself\n",
    "serve non-standard models (e.g. fastText, H2O)\tI can benefit from the full Data Science potential\n",
    "provide custom model dependencies (packages)\tI can run the model the way I expect\n",
    "not be forced to specific development workflow (e.g. providing code in Jupyter Notebooks)\tI can follow standards & good practices I'm used to\n",
    "provide custom code before / after prediction\tI can preprocess & validate data, add callbacks etc.\n",
    "reuse data processing code used for model training\tdata are transformed exactly the same way for both training and prediction\n",
    "test model deployment locally (on my laptop)\tI know if it works fine before the proper deployment\n",
    "have \"1-click / 1-command deployment\" capability\tI can focus on doing Data Science job\n",
    "insure GPU resources for deployed models\tthey can actually work or they can run faster\n",
    "get metadata of deployed model without exposing them to regular clients\tI know what concrete model is used\n",
    "As a ML Engineer, I want to...\tso that...\n",
    "use mature, stable and well-documented tools\tI can rely on them\n",
    "have programmatic access (API, SDK) to the tool\tI can both use CI / CD pipelines to automate deployments and access it locally via CLI\n",
    "log all incoming requests and predictions\tI can use them to debug / monitor / retrain models\n",
    "customize model response\tI can both fulfill system requirements and provide additional, custom metadata\n",
    "provide separate models for different locations / costumers, but hidden behind the same endpoint (== realize multi-model approach)\tI can address complex business needs (e.g. separate NLP model for each local language)\n",
    "version deployed models\tI can introduce new models responsibly\n",
    "handle batch predictions seamlessly\tusers don't have to split large requests into single ones on their own\n",
    "use Request Queue to combine them into batches\tI can optimize usage of resources\n",
    "have my throughput requirements fulfilled\tI can expect predictable response times\n",
    "leverage various deployment strategies (A/B, shadow, canary, blue/green, custom routing etc.)\tI can introduce new models in a well-planned fashion\n",
    "replace model using some config / convenient API\tI can introduce new models in a clear manner\n",
    "schedule model deployments & withdrawals\tI can e.g. withdraw old models after 4 weeks\n",
    "have model instances containerized\tI can customize their environment & deploy them on various Docker-based platforms, especially K8s\n",
    "deploy models with zero downtime\tI can ensure continuity\n",
    "scale model instances\tdeployed models are ready for traffic spikes\n",
    "autoscale model instances\tI don't have to scale them manually\n",
    "have \"scale-to-zero\" capability\tI can save resources and therefore money\n",
    "automatically load balance traffic between existing model instances\tI don't have to worry about downtimes caused by failures / overload of single nodes\n",
    "get a list of deployed models and their status\tI can monitor if they run properly\n",
    "monitor traffic and resource usage\tI have control of deployed models\n",
    "get alerts when anything (incl. both deployment jobs and already deployed models) fails\tI can handle failures quickly\n",
    "manage access to deployed models by having built-in authentication mechanism\tonly expected users will use models and I don't have to develop it by myself\n",
    "As a Model Consumer, I want to...\tso that...\n",
    "have documentation of deployed models\tI know what they do and how to use them\n",
    "see actual performance of model\tI know predictions are faily accurate\n",
    "request predictions using REST API\tI can easily integrate the solution into my system\n",
    "request predictions using GraphQL\tI can easily integrate the solution into my system\n",
    "request predictions using gRPC\tI can easily integrate the solution into my system\n",
    "send data in various formats\tI don't have to transform data on my side\n",
    "choose response format (JSON, YAML, XML etc.)\tI don't have to transform data on my side\n",
    "access model using unchangeable endpoint\tmodel updates don't enforce changes on my side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model repository and deployment\n",
    "- model (blob)\n",
    "-name id etc\n",
    "- input output schema\n",
    "0- list of features\n",
    "- reference to traning and test data sets\n",
    "- buisness metrics\n",
    "-lifecycle status\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in the last few years compiler have been popping up to take whole models described in the api of these frameworks and map them to more efficient hardware than strining through hand optimized function\n",
    "\n",
    "\n",
    "easier option save python model and have scoring be in python service (easy to to set up but can have pickling problems, and performance insssues due to gill etc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch predictions if you dont do online predictions there no prediction latency\n",
    "    - genereate prdeiction in batch offline\n",
    "    - store them sowethere\n",
    "    - pull out pre computed prediction when needed\n",
    "    \n",
    "    problems\n",
    "    - need to know extraclt how many predictions to generate (finite input space )\n",
    "    - cant adapt to changing intrest (finite output space)\n",
    "    - latency\n",
    "    \n",
    "online predictions\n",
    "    - static fgeatures (age gebder hibm ubcine beughtbiorhood)\n",
    "    - dynatmic (what i am reading now watching right now)\n",
    "    \n",
    "    \n",
    "model compressions (quanitzation, knowledge distilation, priunning , etc)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "online learning != online training\n",
    "    online training: learn from each incoming data point\n",
    "        - catastropic forgecting\n",
    "        - can be very expensive\n",
    "    online learning : learn in microbatches\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        traddional ml\n",
    "        \n",
    "        deep learning\n",
    "            onnx tvm (dl prediction serving sysmte)\n",
    "            \n",
    "        ussing hummingbird to mitigate the gap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
